#####################################################################################3
##TF STREAMING
######################################################################################
from pyspark.sql import DataFrame
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, sum, max, min, avg, count, round, lit, year, month, dayofmonth, trim, hour, minute, second, dayofweek
from pyspark.sql.types import StructType, StringType, DoubleType, IntegerType, TimestampType, FloatType
from datetime import datetime
import os 
import pandas as pd
import geopandas as gpd
from shapely.geometry import Point
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import matplotlib.cm as cm
from pyspark.ml.feature import VectorAssembler
from pyspark.ml import Pipeline
from pyspark.ml.regression import RandomForestRegressor
from pyspark.ml.evaluation import RegressionEvaluator

# Crear sesión de Spark
spark = SparkSession \
    .builder \
    .appName("SparkStreamingFromSocket") \
    .master("local[*]") \
    .config("spark.executor.memory", "4g") \
    .config("spark.executor.cores", "4") \
    .config("spark.driver.memory", "4g") \
    .config("spark.sql.shuffle.partitions", "4") \
    .getOrCreate()

# Definir el esquema para los datos JSON que se recibirán
schema = StructType() \
    .add("latitude", DoubleType()) \
    .add("longitude", DoubleType()) \
    .add("date", TimestampType()) \
    .add("customer_id", StringType()) \
    .add("employee_id", StringType()) \
    .add("quantity_products", IntegerType()) \
    .add("order_id", StringType()) \
    .add("commune_code", StringType()) \
    .add("commune_name", StringType()) \
    .add("customer_name", StringType()) \
    .add("employee_name", StringType()) \
    .add("employee_commission", DoubleType())

# Función para guardar los datos recibidos en bronze
def process_data(df, epoch_id):
    try:
        hdfs_path = "/user/root/bronze"
        df.write \
          .format("parquet") \
          .mode("append") \
          .save(hdfs_path)
    except Exception as e:
        print(f"Error al procesar los datos: {e}")

# Leer datos desde el socket
streaming_df = spark \
    .readStream \
    .format("socket") \
    .option("host", "localhost") \
    .option("port", 12345) \
    .load()


# Parsear los datos JSON utilizando el esquema definido
parsed_df = streaming_df \
    .select(from_json(col("value").cast("string"), schema).alias("parsed_value")) \
    .select("parsed_value.*")



# Procesar en streaming y escribir en la capa Silver
query1 = parsed_df \
    .writeStream \
    .trigger(processingTime='2 seconds') \
    .foreachBatch(process_data) \
    .outputMode("append") \
    .start()




############################## CAPA SILVER


# Rutas en hdfs (distrbuido)
bronze_path = "hdfs:///user/root/bronze"  
silver_path = "hdfs:///user/root/silver/unificado.parquet"  

# Función para leer archivos Parquet desde HDFS
def leer_archivos_parquet(path: str) -> DataFrame:
    try:
        # Verificar la existencia del archivo antes de leerlo
        if os.system(f"hdfs dfs -test -e {path}") == 0:
            return spark.read.parquet(path)
        else:
            print(f"El archivo Parquet {path} no existe.")
            return None
    except Exception as e:
        print(f"Error al leer el archivo Parquet {path}: {e}")
        return None
    

# Leer archivos Parquet desde el directorio en HDFS
# Configurar streaming para leer datos desde la capa Bronze
df_bronze = spark \
    .readStream \
    .schema(schema) \
    .format("json") \
    .load(bronze_path)

# Función para agregar una columna con valor constante al precio y dividir la fecha
def transformar_df(df: DataFrame) -> DataFrame:
        df_transformado = df \
            .withColumn("price", lit(3500)) \
            .withColumn("sales", col("quantity_products") * col("price")) \
            .withColumn("commission_value", round(col("sales") * col("employee_commission"), 0)) \
            .withColumn("customer_name", trim(col("customer_name"))) \
            .withColumn("employee_name", trim(col("employee_name"))) \
            .withColumn("commune_name", trim(col("commune_name"))) \
            .withColumn("year", year(col("date"))) \
            .withColumn("month", month(col("date"))) \
            .withColumn("day", dayofmonth(col("date"))) \
            .withColumn("day_week", dayofweek(col("date"))) \
            .withColumn("hour", hour(col("date"))) \
            .withColumn("minute", minute(col("date"))) \
            .withColumn("second", second(col("date")))
        return df_transformado

# Función para guardar DataFrame en un archivo Parquet en la capa Silver, siempre sobreescribe esto es compactar
def guardar_archivo_parquet(df: DataFrame, path: str) -> None:
    df.coalesce(1).write \
        .mode("overwrite") \
        .parquet(path)
        
# Función principal para unir archivos de Bronze a Silver
def unir_archivos_bronze_a_silver(batch_df, batch_id) -> None:
    # Leer archivos Parquet desde la capa Bronze
    df_bronze = leer_archivos_parquet(bronze_path)

    # Transformar el DataFrame de Bronze
    df_transformado = transformar_df(df_bronze)

    # Guardar el DataFrame transformado en la capa Silver
    guardar_archivo_parquet(df_transformado, silver_path)

    # Leer archivos Parquet desde la capa Silver después de transformar
    df_silver_transformado = leer_archivos_parquet(silver_path)

    if df_silver_transformado is not None:
        # Contar la cantidad de registros en la capa Silver después de transformar
        records_processed = df_silver_transformado.count()
        print(f"Cantidad de registros procesados en silver después de transformar: {records_processed}")
    



# Procesar en streaming y escribir en la capa Silver
query2 = df_bronze \
    .writeStream \
    .trigger(processingTime='2 seconds') \
    .foreachBatch(unir_archivos_bronze_a_silver) \
    .outputMode("append") \
    .start()



#### Capa gold
####



# Ruta del archivo Parquet en la capa Silver
silver_path = "hdfs:///user/root/silver/unificado.parquet"

# Definir la ruta en HDFS donde se guardará la tabla en formato Parquet en la capa Gold
gold_path = "hdfs:///user/root/gold/UNALWater"

# Leer los datos desde el archivo Parquet en la capa Silver
#df = spark.read.parquet(silver_path)

schema_gold = StructType() \
    .add("latitude", DoubleType()) \
    .add("longitude", DoubleType()) \
    .add("date", TimestampType()) \
    .add("customer_id", StringType()) \
    .add("employee_id", StringType()) \
    .add("quantity_products", IntegerType()) \
    .add("order_id", StringType()) \
    .add("commune_code", StringType()) \
    .add("commune_name", StringType()) \
    .add("customer_name", StringType()) \
    .add("employee_name", StringType()) \
    .add("employee_commission", DoubleType()) \
    .add("price", IntegerType()) \
    .add("sales", IntegerType()) \
    .add("commission_value", DoubleType()) \
    .add("year", IntegerType()) \
    .add("month", IntegerType()) \
    .add("day", IntegerType()) \
    .add("day_week", IntegerType()) \
    .add("hour", IntegerType()) \
    .add("minute", IntegerType()) \
    .add("second", IntegerType())



df = spark \
    .readStream \
    .schema(schema_gold) \
    .load(silver_path)


# Crear la base de datos si no existe
spark.sql("CREATE DATABASE IF NOT EXISTS UNALwater")
# Establecer la base de datos en uso
spark.sql("USE UNALwater")


# Función principal para unir archivos de Bronze a Silver y ejecutar consultas SQL en streaming
def actualizar_gold(batch_df, batch_id) -> None:
    # Asegurar que la base de datos UNALwater esté en uso
    spark.sql("USE UNALwater")

    # Escribir los datos en la capa Gold
    batch_df.write.mode("append") \
        .partitionBy("date") \
        .format("parquet") \
        .option("path", gold_path) \
        .saveAsTable("UNALWater")
    
    #sparl.sql("DROP DATABASE IF EXISTS UNALwater")
    print("Comportamiento de la cantidad de ventas por comuna:")
    spark.sql("""
    SELECT 
        CASE 
            WHEN commune_name LIKE '%CORREGIMIENTO DE SAN SEBAS%' THEN 'SAN SEBASTIAN DE PALMITAS'
            WHEN commune_name LIKE '%CORREGIMIENTO DE SAN CRIS%' THEN 'SAN CRISTOBAL'
            WHEN commune_name = 'CORREGIMIENTO DE ALTAVISTA' THEN 'ALTAVISTA'
            WHEN commune_name = 'CORREGIMIENTO DE SANTA ELENA' THEN 'SANTA ELENA'
            WHEN commune_name = 'CORREGIMIENTO DE SAN ANTONIO DE PRADO' THEN 'SAN ANTONIO DE PRADO'
        ELSE commune_name END AS Comuna_Corregimiento,
        SUM(quantity_products) AS Cantidad_Productos,
        SUM(sales) AS Total_Ventas
    FROM UNALWater
    GROUP BY Comuna_Corregimiento
    ORDER BY Total_Ventas DESC;
    """).show()
    
    print("Comportamiento de la cantidad de ventas por vendedor:")
    spark.sql("""
    SELECT 
        employee_name AS Vendedor,
        SUM(quantity_products) AS Cantidad_Productos,
        SUM(sales) AS Total_Ventas,
        SUM(commission_value) AS Valor_Comision
    FROM UNALWater
    GROUP BY employee_name
    ORDER BY Valor_Comision DESC
    """).show()
    
    print("Los 10 clientes que más nos han comprado botellas de agua:")
    spark.sql("""
    SELECT 
        customer_name AS Cliente,
        SUM(quantity_products) AS Cantidad_Productos,
        SUM(sales) AS Total_Ventas
    FROM UNALWater
    GROUP BY customer_name
    ORDER BY Total_Ventas DESC
    LIMIT 10
    """).show()
    
    print("Comportamiento de ventas de botellas de agua por día de la semana:")
    spark.sql("""
    SELECT 
        CASE 
            WHEN day_week = 1 THEN 'Domingo' 
            WHEN day_week = 2 THEN 'Lunes' 
            WHEN day_week = 3 THEN 'Martes'
            WHEN day_week = 4 THEN 'Miércoles'
            WHEN day_week = 5 THEN 'Jueves'
            WHEN day_week = 6 THEN 'Viernes'
            WHEN day_week = 7 THEN 'Sábado'
        END AS Dia_Semana,
        SUM(quantity_products) AS Cantidad_Productos,
        SUM(sales) AS Total_Ventas
    FROM UNALWater
    GROUP BY day_week
    ORDER BY Total_Ventas DESC
    """).show()



# Configurar el proceso de escritura de streaming
query3 = df \
    .writeStream \
    .trigger(processingTime='3 seconds') \
    .foreachBatch(actualizar_gold) \
    .outputMode("update") \
    .start()


########################################GRAFICA###################################

# Importar las librerías necesarias



# Definir la ruta del archivo Parquet con datos de ventas en la capa Silver
cargue_inicial_path = 'hdfs:///user/root/silver/unificado.parquet'

def actualizar_grafica(df_cargue, batch_id) -> None:
    try:
        # Cargar el archivo Parquet de cargue inicial como un DataFrame de Spark
        df_cargue = spark.read.parquet(cargue_inicial_path)

        # Convertir las columnas de longitud y latitud a tipo Float
        df_cargue = df_cargue.withColumn('longitude', col('longitude').cast(FloatType())) \
                             .withColumn('latitude', col('latitude').cast(FloatType()))

        # Convertir el DataFrame de Spark en un DataFrame de Pandas
        df_cargue_pd = df_cargue.toPandas()

        # Crear una columna de geometría en el DataFrame de Pandas
        df_cargue_pd['geom'] = df_cargue_pd.apply(lambda row: Point(row['longitude'], row['latitude']), axis=1)

        # Convertir el DataFrame de Pandas en un GeoDataFrame de GeoPandas
        gdf_cargue = gpd.GeoDataFrame(df_cargue_pd, geometry='geom')

        # Cargar el archivo Parquet de geometrías de Medellín como un GeoDataFrame de GeoPandas
        medellin_neighborhoods = gpd.read_parquet('./data/medellin_neighborhoods.parquet')

        # Crear una escala de colores basada en las ventas
        norm = mcolors.Normalize(vmin=gdf_cargue['sales'].min(), vmax=gdf_cargue['sales'].max())
        cmap = cm.ScalarMappable(norm=norm, cmap='viridis')

        # Asignar colores a cada punto basado en las ventas
        gdf_cargue['color'] = gdf_cargue['sales'].apply(lambda x: cmap.to_rgba(x))

        # Crear el gráfico con las geometrías de Medellín y los puntos del cargue inicial
        fig, ax = plt.subplots(figsize=(15, 15))
        medellin_neighborhoods.plot(ax=ax, color='lightgrey', edgecolor='darkblue')

        # Graficar los puntos con tamaño proporcional a las ventas y colores variados
        gdf_cargue.plot(ax=ax, color=gdf_cargue['color'], markersize=gdf_cargue['sales'] / 100, alpha=0.6)

        plt.title('Datos de ventas en Medellín y ubicaciones de clientes', fontsize = 16)
        plt.gca().spines['top'].set_visible(False)
        plt.gca().spines['left'].set_visible(False)
        plt.gca().spines['right'].set_visible(False)
        plt.gca().spines['bottom'].set_visible(False)
        plt.gca().axes.get_yaxis().set_visible(False)
        plt.gca().axes.get_xaxis().set_visible(False)
        plt.grid(False)

        # Añadir la barra de color
        cax = fig.add_axes([0.95, 0.45, 0.02, 0.3])
        cbar = plt.colorbar(cmap, cax=cax, orientation='vertical', label='Ventas')
        plt.show()

        # Guardar la figura en un archivo si la visualización es correcta
        plt.savefig('./medellin_neighborhoods_simulacion.png')
        plt.close()

    except Exception as e:
        print(f"Error al convertir o visualizar los datos con GeoPandas: {str(e)}")

# Configurar el proceso de escritura de streaming
query4 = df \
    .writeStream \
    .trigger(processingTime='6 seconds') \
    .foreachBatch(actualizar_grafica) \
    .outputMode("update") \
    .start()

########################################MACHINE LEARNING

# Datos en Spark




# Definir la ruta del archivo Parquet con datos de ventas en la capa Silver
cargue_inicial_path = 'hdfs:///user/root/silver/unificado.parquet'

def actualizar_ml(df_cargue, batch_id) -> None:

    # Convertir las columnas de longitud y latitud a tipo Float y crear columna 'geom'
    df_cargue = df_cargue.withColumn('longitude', col('longitude').cast(FloatType())) \
                             .withColumn('latitude', col('latitude').cast(FloatType()))

    # Seleccionar las columnas necesarias
    df_pronostico = df_cargue.select('latitude', 'longitude', 'sales', 'commune_name')

    # Mostrar el DataFrame
    df_pronostico.show(truncate=100)

# Configurar el proceso de escritura de streaming
query5 = df \
    .writeStream \
    .trigger(processingTime='5 seconds') \
    .foreachBatch(actualizar_ml) \
    .outputMode("update") \
    .start()


# Esperar a que las consultas de streaming terminen
query1.awaitTermination()
query2.awaitTermination()
query3.awaitTermination()
query4.awaitTermination()
query5.awaitTermination()
