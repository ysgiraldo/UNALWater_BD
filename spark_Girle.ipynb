{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27c011c3-185f-45d1-a9bf-403fe7752cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (1.4.2)\n",
      "Requirement already satisfied: geopandas in /usr/local/lib/python3.8/dist-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (3.5.2)\n",
      "Requirement already satisfied: faker in /usr/local/lib/python3.8/dist-packages (13.12.1)\n",
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.8/dist-packages (3.1.3)\n",
      "Requirement already satisfied: pyarrow in /usr/local/lib/python3.8/dist-packages (8.0.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: fiona>=1.8.19 in /usr/local/lib/python3.8/dist-packages (from geopandas) (1.9.6)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from geopandas) (21.3)\n",
      "Requirement already satisfied: shapely>=1.7.1 in /usr/local/lib/python3.8/dist-packages (from geopandas) (2.0.4)\n",
      "Requirement already satisfied: pyproj>=3.0.1 in /usr/local/lib/python3.8/dist-packages (from geopandas) (3.5.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (9.1.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (1.4.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (4.33.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: py4j==0.10.9 in /usr/local/lib/python3.8/dist-packages (from pyspark) (0.10.9)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from fiona>=1.8.19->geopandas) (2022.5.18.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.8/dist-packages (from fiona>=1.8.19->geopandas) (21.4.0)\n",
      "Requirement already satisfied: cligj>=0.5 in /usr/local/lib/python3.8/dist-packages (from fiona>=1.8.19->geopandas) (0.7.2)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from fiona>=1.8.19->geopandas) (4.11.4)\n",
      "Requirement already satisfied: click-plugins>=1.0 in /usr/local/lib/python3.8/dist-packages (from fiona>=1.8.19->geopandas) (1.1.1)\n",
      "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.8/dist-packages (from fiona>=1.8.19->geopandas) (8.1.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from fiona>=1.8.19->geopandas) (1.16.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->fiona>=1.8.19->geopandas) (3.8.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 177, in emit\n",
      "    self.console.print(renderable, overflow=\"ignore\", crop=False, style=style)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1752, in print\n",
      "    extend(render(renderable, render_options))\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1390, in render\n",
      "    for render_output in iter_render:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 134, in __rich_console__\n",
      "    for line in lines:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/segment.py\", line 245, in split_lines\n",
      "    for segment in segments:\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/console.py\", line 1368, in render\n",
      "    renderable = rich_cast(renderable)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_vendor/rich/protocol.py\", line 36, in rich_cast\n",
      "    renderable = cast_method()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/self_outdated_check.py\", line 130, in __rich__\n",
      "    pip_cmd = get_best_invocation_for_this_pip()\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/entrypoints.py\", line 58, in get_best_invocation_for_this_pip\n",
      "    if found_executable and os.path.samefile(\n",
      "  File \"/usr/lib/python3.8/genericpath.py\", line 101, in samefile\n",
      "    s2 = os.stat(f2)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/usr/bin/pip3.8'\n",
      "Call stack:\n",
      "  File \"/usr/local/bin/pip\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/main.py\", line 70, in main\n",
      "    return command.main(cmd_args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 101, in main\n",
      "    return self._main(args)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/base_command.py\", line 223, in _main\n",
      "    self.handle_pip_version_check(options)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/cli/req_command.py\", line 148, in handle_pip_version_check\n",
      "    pip_self_version_check(session, options)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/self_outdated_check.py\", line 237, in pip_self_version_check\n",
      "    logger.info(\"[present-rich] %s\", upgrade_prompt)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1446, in info\n",
      "    self._log(INFO, msg, args, **kwargs)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1589, in _log\n",
      "    self.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1599, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 1661, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 954, in handle\n",
      "    self.emit(record)\n",
      "  File \"/usr/local/lib/python3.8/dist-packages/pip/_internal/utils/logging.py\", line 179, in emit\n",
      "    self.handleError(record)\n",
      "Message: '[present-rich] %s'\n",
      "Arguments: (UpgradePrompt(old='22.1.2', new='24.0'),)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas geopandas matplotlib faker pyspark pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e5ea5b-d9d6-47f2-a488-e02cd7d7c355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----+-----------+-----------+-----------------+--------+------------+------------+-------------+-------------+-------------------+\n",
      "|latitude|longitude|date|customer_id|employee_id|quantity_products|order_id|commune_code|commune_name|customer_name|employee_name|employee_commission|\n",
      "+--------+---------+----+-----------+-----------+-----------------+--------+------------+------------+-------------+-------------+-------------------+\n",
      "+--------+---------+----+-----------+-----------+-----------------+--------+------------+------------+-------------+-------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+-------------------+-----------+-----------+-----------------+----------+------------+------------------------------------------+------------------+-----------------+-------------------+\n",
      "|latitude          |longitude         |date               |customer_id|employee_id|quantity_products|order_id  |commune_code|commune_name                              |customer_name     |employee_name    |employee_commission|\n",
      "+------------------+------------------+-------------------+-----------+-----------+-----------------+----------+------------+------------------------------------------+------------------+-----------------+-------------------+\n",
      "|6.230073881503961 |-75.4992337499913 |2024-02-09 06:15:09|5041       |6337       |64               |463624432 |90          |CORREGIMIENTO DE SANTA ELENA              |Benedict Underwood|Phyllis Hubbard  |0.1                |\n",
      "|6.248935218793185 |-75.61616199766836|2023-01-04 16:52:51|3690       |5668       |85               |6470520831|13          |SAN JAVIER                                |Rhonda Daugherty  |Melanie Ball     |0.18               |\n",
      "|6.299548810920262 |-75.68871921963368|2023-07-11 20:36:52|6742       |1561       |88               |5310008176|50          |CORREGIMIENTO DE SAN SEBASTIÁN DE PALMITAS|Erich Haley       |Amelia Nolan     |0.09               |\n",
      "|6.173579120452036 |-75.5396649710202 |2024-06-15 17:49:27|4039       |3455       |26               |9378855455|90          |CORREGIMIENTO DE SANTA ELENA              |Edward Armstrong  |Davis Jenkins    |0.13               |\n",
      "|6.2448041343745375|-75.59476160343198|2024-04-05 00:15:28|2918       |3455       |51               |1892779357|11          |LAURELES ESTADIO                          |Haviva Kinney     |Davis Jenkins    |0.13               |\n",
      "|6.296107941441933 |-75.61114487518773|2023-08-28 11:30:39|8467       |1482       |81               |9150923103|60          |CORREGIMIENTO DE SAN CRISTÓBAL            |Fallon Lang       |Elijah Parker    |0.13               |\n",
      "|6.354947624012999 |-75.66640892691781|2023-10-20 13:01:44|4686       |6659       |25               |3845015883|50          |CORREGIMIENTO DE SAN SEBASTIÁN DE PALMITAS|Avram Jenkins     |Melinda Le       |0.06               |\n",
      "|6.2238666758776535|-75.52304346670228|2023-09-07 21:54:09|9680       |6337       |60               |9992083622|90          |CORREGIMIENTO DE SANTA ELENA              |Wendy Russell     |Phyllis Hubbard  |0.1                |\n",
      "|6.292136451634361 |-75.67320131435325|2023-05-03 16:23:42|5875       |6659       |62               |4242651374|50          |CORREGIMIENTO DE SAN SEBASTIÁN DE PALMITAS|Jackson Frank     |Melinda Le       |0.06               |\n",
      "|6.209810235563648 |-75.65508653382972|2023-12-29 13:06:33|5167       |3830       |57               |5107709912|80          |CORREGIMIENTO DE SAN ANTONIO DE PRADO     |Owen Moon         |Shaeleigh Turner |0.06               |\n",
      "|6.285924690080576 |-75.65676516465778|2023-10-18 06:35:49|7881       |1679       |95               |8230285744|60          |CORREGIMIENTO DE SAN CRISTÓBAL            |Reece Griffith    |Darius Greer     |0.04               |\n",
      "|6.219595187924061 |-75.67451669812422|2023-09-24 21:57:04|5550       |6696       |25               |2005491702|80          |CORREGIMIENTO DE SAN ANTONIO DE PRADO     |Caldwell Williams |Patricia Cox     |0.04               |\n",
      "|6.267694752802599 |-75.63568423214271|2024-01-29 04:22:46|9004       |4750       |89               |9108414652|60          |CORREGIMIENTO DE SAN CRISTÓBAL            |Christen Potts    |Howard Guthrie   |0.16               |\n",
      "|6.297175252919792 |-75.51246071039607|2023-10-13 03:40:53|7442       |4942       |36               |2536364325|90          |CORREGIMIENTO DE SANTA ELENA              |MacKensie Frye    |Sydnee Kirby     |0.16               |\n",
      "|6.338956733157524 |-75.69359717348732|2024-03-15 07:12:53|4207       |9726       |55               |1822931716|50          |CORREGIMIENTO DE SAN SEBASTIÁN DE PALMITAS|Alan Dennis       |Christen Hamilton|0.17               |\n",
      "+------------------+------------------+-------------------+-----------+-----------+-----------------+----------+------------+------------------------------------------+------------------+-----------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+-------------------+-----------+-----------+-----------------+----------+------------+-------------------------------------+------------------+---------------+-------------------+\n",
      "|latitude         |longitude         |date               |customer_id|employee_id|quantity_products|order_id  |commune_code|commune_name                         |customer_name     |employee_name  |employee_commission|\n",
      "+-----------------+------------------+-------------------+-----------+-----------+-----------------+----------+------------+-------------------------------------+------------------+---------------+-------------------+\n",
      "|6.178295435372668|-75.53388470694372|2024-06-03 03:38:02|8033       |5668       |29               |2843614140|90          |CORREGIMIENTO DE SANTA ELENA         |Justine Burch     |Melanie Ball   |0.18               |\n",
      "|6.251426142581339|-75.60798102540919|2024-02-16 18:21:31|7062       |9438       |98               |5374211788|12          |LA AMÉRICA                           |Roth Coffey       |Dacey Barr     |0.02               |\n",
      "|6.195763161366637|-75.6836998694335 |2024-01-02 22:09:09|3271       |1473       |48               |5188984143|80          |CORREGIMIENTO DE SAN ANTONIO DE PRADO|Driscoll Castaneda|Celeste Johns  |0.07               |\n",
      "|6.200026707884418|-75.56574689327938|2023-11-22 19:58:58|3392       |1679       |43               |4073718245|14          |EL POBLADO                           |Madonna Kinney    |Darius Greer   |0.04               |\n",
      "|6.294485925740862|-75.54135161237387|2024-05-20 23:03:54|9630       |5668       |94               |1639774531|01          |POPULAR                              |Samantha Huff     |Melanie Ball   |0.18               |\n",
      "|6.244278650046826|-75.70435808870175|2024-01-06 23:19:29|1317       |5668       |76               |7746003091|80          |CORREGIMIENTO DE SAN ANTONIO DE PRADO|Herrod Bonner     |Melanie Ball   |0.18               |\n",
      "|6.177281003612507|-75.53421858945586|2024-05-05 15:32:51|7305       |8362       |59               |4398455809|90          |CORREGIMIENTO DE SANTA ELENA         |Brock Hahn        |Catherine King |0.07               |\n",
      "|6.240945597795029|-75.67912758866865|2023-01-09 12:13:49|1669       |9438       |53               |163459778 |80          |CORREGIMIENTO DE SAN ANTONIO DE PRADO|Clarke Bonner     |Dacey Barr     |0.02               |\n",
      "|6.237292299112045|-75.49767531144414|2023-11-22 06:50:23|3984       |6337       |77               |5478463077|90          |CORREGIMIENTO DE SANTA ELENA         |Avram Benjamin    |Phyllis Hubbard|0.1                |\n",
      "|6.250750869172491|-75.5777880490254 |2024-05-30 18:05:27|4431       |2232       |47               |4333846629|10          |LA CANDELARIA                        |Brett Burgess     |Forrest Bradley|0.07               |\n",
      "|6.168944999402265|-75.65143990927908|2023-11-30 14:25:07|3946       |1482       |66               |3346788095|80          |CORREGIMIENTO DE SAN ANTONIO DE PRADO|Kathleen Mckee    |Elijah Parker  |0.13               |\n",
      "|6.217438315450414|-75.6706652507939 |2024-02-07 07:57:32|1542       |4750       |65               |9715356419|80          |CORREGIMIENTO DE SAN ANTONIO DE PRADO|Howard Byrd       |Howard Guthrie |0.16               |\n",
      "|6.182806256891605|-75.68192039272803|2023-05-28 18:19:27|9090       |3455       |98               |8038301061|80          |CORREGIMIENTO DE SAN ANTONIO DE PRADO|Emerson Gonzales  |Davis Jenkins  |0.13               |\n",
      "|6.212805707952736|-75.51634788664855|2023-02-15 15:05:53|9585       |1679       |61               |8650116448|90          |CORREGIMIENTO DE SANTA ELENA         |Lamar Pace        |Darius Greer   |0.04               |\n",
      "|6.239876492728885|-75.54284317562926|2023-09-02 02:53:57|7775       |5668       |29               |4842750276|08          |VILLA HERMOSA                        |Kay Garner        |Melanie Ball   |0.18               |\n",
      "|6.209541531810181|-75.6146489458082 |2023-06-24 07:03:21|8740       |4750       |55               |7900480255|70          |CORREGIMIENTO DE ALTAVISTA           |Christen Hopkins  |Howard Guthrie |0.16               |\n",
      "|6.275552025613368|-75.69023346760808|2023-01-26 02:26:04|3251       |6659       |36               |5902075870|80          |CORREGIMIENTO DE SAN ANTONIO DE PRADO|Marcia Koch       |Melinda Le     |0.06               |\n",
      "|6.244216220690059|-75.64127828713805|2024-04-10 00:27:25|8048       |6696       |53               |7956012246|70          |CORREGIMIENTO DE ALTAVISTA           |Morgan Pennington |Patricia Cox   |0.04               |\n",
      "|6.231533783387035|-75.69280355889026|2023-12-30 10:04:23|8190       |1473       |28               |7387068984|80          |CORREGIMIENTO DE SAN ANTONIO DE PRADO|Zephania Page     |Celeste Johns  |0.07               |\n",
      "|6.261613277234854|-75.6815168273611 |2023-12-02 02:37:21|4499       |9435       |83               |5992776161|80          |CORREGIMIENTO DE SAN ANTONIO DE PRADO|Camden Brown      |Ryan Nichols   |0.18               |\n",
      "+-----------------+------------------+-------------------+-----------+-----------+-----------------+----------+------------+-------------------------------------+------------------+---------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StringType, DoubleType, IntegerType, TimestampType\n",
    "\n",
    "# Crear sesión de Spark\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"SparkStreamingFromSocket\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Definir el esquema para los datos JSON que se recibirán\n",
    "schema = StructType() \\\n",
    "    .add(\"latitude\", DoubleType()) \\\n",
    "    .add(\"longitude\", DoubleType()) \\\n",
    "    .add(\"date\", TimestampType()) \\\n",
    "    .add(\"customer_id\", StringType()) \\\n",
    "    .add(\"employee_id\", StringType()) \\\n",
    "    .add(\"quantity_products\", IntegerType()) \\\n",
    "    .add(\"order_id\", StringType()) \\\n",
    "    .add(\"commune_code\", StringType()) \\\n",
    "    .add(\"commune_name\", StringType()) \\\n",
    "    .add(\"customer_name\", StringType()) \\\n",
    "    .add(\"employee_name\", StringType()) \\\n",
    "    .add(\"employee_commission\", DoubleType())\n",
    "\n",
    "# Leer datos desde el socket\n",
    "streaming_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 12345) \\\n",
    "    .load()\n",
    "\n",
    "# Parsear los datos JSON utilizando el esquema definido\n",
    "parsed_df = streaming_df \\\n",
    "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"parsed_value\")) \\\n",
    "    .select(\"parsed_value.*\")\n",
    "\n",
    "# Función para guardar los datos recibidos en bronze\n",
    "def process_data(df, epoch_id):\n",
    "    try:\n",
    "        hdfs_path = \"/user/root/bronze\"\n",
    "        df.write \\\n",
    "          .format(\"parquet\") \\\n",
    "          .mode(\"append\") \\\n",
    "          .save(hdfs_path)\n",
    "        df.show(truncate=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Error al procesar los datos: {e}\")\n",
    "        \n",
    "# Escribir los resultados en la consola\n",
    "query = parsed_df \\\n",
    "    .writeStream \\\n",
    "    .foreachBatch(process_data) \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "# Mantener el stream en ejecución\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef945523-5155-477f-83b1-3325e81b1a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de registros procesados en silver después de transformar: 590\n",
      "root\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- employee_id: string (nullable = true)\n",
      " |-- quantity_products: integer (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- commune_code: string (nullable = true)\n",
      " |-- commune_name: string (nullable = true)\n",
      " |-- customer_name: string (nullable = true)\n",
      " |-- employee_name: string (nullable = true)\n",
      " |-- employee_commission: double (nullable = true)\n",
      " |-- price: integer (nullable = false)\n",
      " |-- sales: integer (nullable = true)\n",
      " |-- commission_value: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      "\n",
      "+------------------+------------------+-------------------+-----------+-----------+-----------------+----------+------------+--------------------+-----------------+---------------+-------------------+-----+------+----------------+----+-----+---+\n",
      "|          latitude|         longitude|               date|customer_id|employee_id|quantity_products|  order_id|commune_code|        commune_name|    customer_name|  employee_name|employee_commission|price| sales|commission_value|year|month|day|\n",
      "+------------------+------------------+-------------------+-----------+-----------+-----------------+----------+------------+--------------------+-----------------+---------------+-------------------+-----+------+----------------+----+-----+---+\n",
      "| 6.215337432631484| -75.4906691755128|2023-09-13 23:04:10|       2566|       1114|               95|9122463315|          90|CORREGIMIENTO DE ...|       Jason Sims|  Bevis Sanford|               0.14| 3500|332500|         46550.0|2023|    9| 13|\n",
      "| 6.207564568151786| -75.6113815736173|2023-07-27 01:28:23|       3251|       1561|               89|3185212543|          70|CORREGIMIENTO DE ...|      Marcia Koch|   Amelia Nolan|               0.09| 3500|311500|         28035.0|2023|    7| 27|\n",
      "| 6.364326843393114|-75.68919110963314|2024-01-04 14:03:30|       1317|       1737|               72|8850500691|          50|CORREGIMIENTO DE ...|    Herrod Bonner|Althea Mckenzie|               0.13| 3500|252000|         32760.0|2024|    1|  4|\n",
      "| 6.210512530874288|-75.62278130238026|2024-01-24 17:18:54|       2506|       3455|               26|8407225051|          70|CORREGIMIENTO DE ...|   Ignatius Knapp|  Davis Jenkins|               0.13| 3500| 91000|         11830.0|2024|    1| 24|\n",
      "| 6.302195692398886|-75.64643307015403|2023-07-01 09:45:12|       5276|       1679|               87|1603507685|          60|CORREGIMIENTO DE ...|      Rajah Burke|   Darius Greer|               0.04| 3500|304500|         12180.0|2023|    7|  1|\n",
      "| 6.230235018402578|-75.65182163285905|2023-04-08 02:08:00|       7610|       6659|               85|9519868498|          70|CORREGIMIENTO DE ...|    Jamalia Walsh|     Melinda Le|               0.06| 3500|297500|         17850.0|2023|    4|  8|\n",
      "| 6.179856146055103|-75.66791114410925|2024-06-03 05:14:42|       5201|       8362|               36|1852161937|          80|CORREGIMIENTO DE ...|      Magee Yates| Catherine King|               0.07| 3500|126000|          8820.0|2024|    6|  3|\n",
      "| 6.336038221202781|-75.69235314585879|2023-03-10 22:17:14|       5139|       1679|               91|4709980891|          50|CORREGIMIENTO DE ...|      Ira Mcclure|   Darius Greer|               0.04| 3500|318500|         12740.0|2023|    3| 10|\n",
      "|6.2594649538347005|-75.63525403442436|2024-03-07 11:39:44|       7674|       6337|               54|3304208857|          60|CORREGIMIENTO DE ...|     Sierra Frank|Phyllis Hubbard|                0.1| 3500|189000|         18900.0|2024|    3|  7|\n",
      "| 6.355395913730829|-75.67518409075872|2023-06-30 01:27:55|       5065|       1561|               93|4588040238|          50|CORREGIMIENTO DE ...|    Fallon Vinson|   Amelia Nolan|               0.09| 3500|325500|         29295.0|2023|    6| 30|\n",
      "| 6.228373933014193|-75.62554160986069|2023-04-13 19:30:45|       3787|       5668|               51|2723691106|          70|CORREGIMIENTO DE ...|    Garrett Beach|   Melanie Ball|               0.18| 3500|178500|         32130.0|2023|    4| 13|\n",
      "|  6.27357891888737|-75.66930786127887|2023-12-02 18:18:19|       7530|       4942|               68|4992941284|          60|CORREGIMIENTO DE ...|Prescott Hatfield|   Sydnee Kirby|               0.16| 3500|238000|         38080.0|2023|   12|  2|\n",
      "| 6.259744193382541|-75.58789422367357|2024-05-27 09:01:25|       4386|       6337|               54|2390201742|          11|    LAURELES ESTADIO| Candace Humphrey|Phyllis Hubbard|                0.1| 3500|189000|         18900.0|2024|    5| 27|\n",
      "| 6.197198278168543|-75.48589091654138|2023-04-27 16:25:56|       4180|       3455|               71|3688918847|          90|CORREGIMIENTO DE ...|    Destiny Kirby|  Davis Jenkins|               0.13| 3500|248500|         32305.0|2023|    4| 27|\n",
      "| 6.248162196269105|-75.61152987548789|2023-02-15 01:00:40|       8432|       6337|               74|5780346763|          12|          LA AMÉRICA|  Priscilla Munoz|Phyllis Hubbard|                0.1| 3500|259000|         25900.0|2023|    2| 15|\n",
      "|6.2114551484314635|-75.60340856364947|2023-04-29 01:16:34|       4039|       1482|               26|8482981049|          16|               BELÉN| Edward Armstrong|  Elijah Parker|               0.13| 3500| 91000|         11830.0|2023|    4| 29|\n",
      "|   6.2391368758677|-75.60869517818428|2024-04-04 11:49:47|       2693|       8362|               67|2249492636|          11|    LAURELES ESTADIO|     Harper Hodge| Catherine King|               0.07| 3500|234500|         16415.0|2024|    4|  4|\n",
      "| 6.207751019685932|-75.69759015929634|2024-01-25 05:02:03|       8505|       9438|               35|3863817898|          80|CORREGIMIENTO DE ...|  Theodore Massey|     Dacey Barr|               0.02| 3500|122500|          2450.0|2024|    1| 25|\n",
      "| 6.282860667968353|-75.60163299124291|2024-05-12 20:08:37|       1923|       1737|               51| 475983923|          07|             ROBLEDO|  Hammett Swanson|Althea Mckenzie|               0.13| 3500|178500|         23205.0|2024|    5| 12|\n",
      "| 6.267014600658284|-75.49935036434846|2023-04-15 12:55:13|       3167|       1679|               91|7029735324|          90|CORREGIMIENTO DE ...|     Vera Solomon|   Darius Greer|               0.04| 3500|318500|         12740.0|2023|    4| 15|\n",
      "+------------------+------------------+-------------------+-----------+-----------+-----------------+----------+------------+--------------------+-----------------+---------------+-------------------+-----+------+----------------+----+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, expr, round, lit, year, month, dayofmonth, trim\n",
    "import os \n",
    "\n",
    "# Obtener la sesión de Spark existente si está activa\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Función para leer archivos Parquet desde HDFS\n",
    "def leer_archivos_parquet(path: str) -> DataFrame:\n",
    "    try:\n",
    "        # Verificar la existencia del archivo antes de leerlo\n",
    "        if os.system(f\"hdfs dfs -test -e {path}\") == 0:\n",
    "            return spark.read.parquet(path)\n",
    "        else:\n",
    "            print(f\"El archivo Parquet {path} no existe.\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error al leer el archivo Parquet {path}: {e}\")\n",
    "        return None\n",
    "    \n",
    "# Ruta de bronze\n",
    "bronze_path = \"hdfs:///user/root/bronze\"  \n",
    "\n",
    "# Leer archivos Parquet desde el directorio en HDFS\n",
    "df_bronze = leer_archivos_parquet(bronze_path)\n",
    "\n",
    "# Función para agregar una columna con valor constante al precio y dividir la fecha\n",
    "def transformar_df(df: DataFrame) -> DataFrame:\n",
    "        df_transformado = df \\\n",
    "            .withColumn(\"price\", lit(3500)) \\\n",
    "            .withColumn(\"sales\", col(\"quantity_products\") * col(\"price\")) \\\n",
    "            .withColumn(\"commission_value\", round(col(\"sales\") * col(\"employee_commission\"), 0)) \\\n",
    "            .withColumn(\"customer_name\", trim(col(\"customer_name\"))) \\\n",
    "            .withColumn(\"employee_name\", trim(col(\"employee_name\"))) \\\n",
    "            .withColumn(\"commune_name\", trim(col(\"commune_name\"))) \\\n",
    "            .withColumn(\"year\", year(col(\"date\"))) \\\n",
    "            .withColumn(\"month\", month(col(\"date\"))) \\\n",
    "            .withColumn(\"day\", dayofmonth(col(\"date\")))\n",
    "        return df_transformado\n",
    "\n",
    "# Función para guardar DataFrame en un archivo Parquet en la capa Silver, siempre sobreescribe\n",
    "def guardar_archivo_parquet(df: DataFrame, path: str) -> None:\n",
    "        df.coalesce(1).write.mode(\"overwrite\").parquet(path)\n",
    "\n",
    "# Función principal para unir archivos de Bronze a Silver\n",
    "def unir_archivos_bronze_a_silver(bronze_path: str, silver_path: str) -> None:\n",
    "    # Leer archivos Parquet desde la capa Bronze\n",
    "    df_bronze = leer_archivos_parquet(bronze_path)\n",
    "\n",
    "    # Transformar el DataFrame de Bronze\n",
    "    df_transformado = transformar_df(df_bronze)\n",
    "\n",
    "    # Guardar el DataFrame transformado en la capa Silver\n",
    "    guardar_archivo_parquet(df_transformado, silver_path)\n",
    "\n",
    "    # Leer archivos Parquet desde la capa Silver después de transformar\n",
    "    df_silver_transformado = leer_archivos_parquet(silver_path)\n",
    "\n",
    "    if df_silver_transformado is not None:\n",
    "        # Contar la cantidad de registros en la capa Silver después de transformar\n",
    "        records_processed = df_silver_transformado.count()\n",
    "        print(f\"Cantidad de registros procesados en silver después de transformar: {records_processed}\")\n",
    "    \n",
    "    # Mostrar el DataFrame transformado (opcional)\n",
    "    df_transformado.printSchema()\n",
    "    df_transformado.show()\n",
    "\n",
    "# Rutas de ejemplo ajustadas\n",
    "bronze_path = \"hdfs:///user/root/bronze\"  \n",
    "silver_path = \"hdfs:///user/root/silver/unificado.parquet\"  \n",
    "\n",
    "# Ejecutar el proceso de unión\n",
    "unir_archivos_bronze_a_silver(bronze_path, silver_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8a7a001f-050e-4e8f-b019-ab1c11ca0984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   1 root supergroup    2287806 2024-06-17 04:19 /user/root/bronze/medellin_neighborhoods.parquet\n"
     ]
    }
   ],
   "source": [
    "#!hdfs dfs -ls /user/root/silver/unificado.parquet\n",
    "#!hdfs dfs -mkdir -p /user/root/gold\n",
    "#!hdfs dfs -ls /user/root/bronze\n",
    "#!hdfs dfs -copyFromLocal medellin_neighborhoods.parquet /user/root/bronze\n",
    "#!hdfs dfs -ls /user/root/silver\n",
    "#!hdfs dfs -ls /user/root/gold\n",
    "#!hdfs dfs -ls /user/root/bronze\n",
    "!hdfs dfs -ls /user/root/bronze | grep medellin_neighborhoods.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57988c48-78c4-46a2-a49b-47e97af30764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error al convertir o visualizar los datos con GeoPandas: An error occurred while calling o568.showString.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 45.0 failed 1 times, most recent failure: Lost task 0.0 in stage 45.0 (TID 45) (0f90e151db83 executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for shapely.io.from_wkb)\n",
      "\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n",
      "\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2303)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2252)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2251)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2251)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1124)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1124)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2490)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2432)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2421)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:902)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:390)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3709)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2735)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3700)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3698)\n",
      "\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2735)\n",
      "\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2942)\n",
      "\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:302)\n",
      "\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:339)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for shapely.io.from_wkb)\n",
      "\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n",
      "\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/17 04:46:44 ERROR executor.Executor: Exception in task 0.0 in stage 45.0 (TID 45)\n",
      "net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for shapely.io.from_wkb)\n",
      "\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n",
      "\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:484)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:490)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:345)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:898)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:498)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:501)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "24/06/17 04:46:44 ERROR scheduler.TaskSetManager: Task 0 in stage 45.0 failed 1 times; aborting job\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Crear la sesión de Spark con configuración ajustada\n",
    "spark = SparkSession.builder \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Definir la ruta del archivo Parquet con datos de ventas en la capa Silver\n",
    "cargue_inicial_path = 'hdfs:///user/root/silver/unificado.parquet'\n",
    "\n",
    "try:\n",
    "    # Cargar el archivo Parquet de cargue inicial como un DataFrame de Spark\n",
    "    df_cargue = spark.read.parquet(cargue_inicial_path).limit(10)  # Limitar a 10 registros para pruebas\n",
    "\n",
    "    # Definir la función Python para crear puntos a partir de latitud y longitud\n",
    "    def create_point(longitude, latitude):\n",
    "        return Point(float(longitude), float(latitude))\n",
    "\n",
    "    # Registrar la función como una UDF (User Defined Function) en Spark\n",
    "    create_point_udf = udf(create_point, returnType=FloatType())\n",
    "\n",
    "    # Convertir las columnas de longitud y latitud a tipo Float y crear columna 'geom'\n",
    "    df_cargue = df_cargue.withColumn('longitude', col('longitude').cast(FloatType())) \\\n",
    "                         .withColumn('latitude', col('latitude').cast(FloatType())) \\\n",
    "                         .withColumn('geom', create_point_udf(col('longitude'), col('latitude')))\n",
    "    df_cargue.show()\n",
    "#     # Convertir el DataFrame de Spark en un GeoDataFrame de GeoPandas\n",
    "#     gdf_cargue = gpd.GeoDataFrame(df_cargue.toPandas(), geometry='geom')\n",
    "    \n",
    "#     # Imprimir las primeras filas para verificar la conversión\n",
    "#     print(\"Primeras filas de GeoDataFrame:\")\n",
    "#     print(gdf_cargue.head())\n",
    "\n",
    "#     # Definir la ruta del archivo Parquet con geometrías de Medellín en la capa Bronze\n",
    "#     medellin_neighborhoods_path = 'hdfs:///user/root/bronze/medellin_neighborhoods.parquet'\n",
    "    \n",
    "#     # Cargar el archivo Parquet de geometrías de Medellín como un GeoDataFrame de GeoPandas\n",
    "#     medellin_neighborhoods = gpd.read_parquet(medellin_neighborhoods_path)\n",
    "    \n",
    "#     # Imprimir las primeras filas para verificar la carga de datos\n",
    "#     print(\"\\nPrimeras filas de medellin_neighborhoods GeoDataFrame:\")\n",
    "#     print(medellin_neighborhoods.head())\n",
    "\n",
    "# #     # Crear el gráfico con las geometrías de Medellín y los puntos del cargue inicial\n",
    "# #     fig, ax = plt.subplots(figsize=(20, 20))\n",
    "# #     medellin_neighborhoods.plot(ax=ax, color='lightgrey', edgecolor='darkblue')\n",
    "# #     gdf_cargue.plot(ax=ax, color='blue', markersize=10, alpha=0.6)\n",
    "\n",
    "# #     plt.title('Datos simulados de ventas en Medellín y ubicaciones de clientes')\n",
    "# #     plt.xlabel('Longitud')\n",
    "# #     plt.ylabel('Latitud')\n",
    "# #     plt.grid(True)\n",
    "# #     plt.show()\n",
    "    \n",
    "# #     # Guardar la figura en un archivo si la visualización es correcta\n",
    "# #     plt.savefig('./data/medellin_neighborhoods_simulacion.png')\n",
    "# #     plt.close()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al convertir o visualizar los datos con GeoPandas: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2131524-8aa5-44b3-80ad-0e1f2409dbd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
